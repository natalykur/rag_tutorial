{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natalykur/rag_tutorial/blob/main/Basic_rag_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell installs the necessary Python libraries used in the notebook:\n",
        "- `openai`: To interact with OpenAI models.\n",
        "- `pymupdf` (imported as `fitz`): For **loading **and processing PDF files.\n",
        "- `faiss-cpu`: A library for fast similarity search and clustering of dense vectors (used to store **embeddings**).\n",
        "- `scikit-learn`: Required for certain **vector store **operations and utilities.\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "6HbPIigeBITu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG - general problem**\n",
        "\n",
        "Language models often give outdated or incorrect answers because they can’t access external or real-time information. RAG solves this by retrieving relevant context from trusted sources, making responses more accurate and reliable.\n",
        "\n"
      ],
      "metadata": {
        "id": "b-nVCh9_SutV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG - let's understand the problem**\n",
        "\n",
        "Adding the document directly to the prompt context isn't enough because:\n",
        "\n",
        "1. **Token Limits** – Language models have a maximum input size. Long documents often exceed this limit, forcing truncation and loss of important information.\n",
        "\n",
        "2. **Noisy or Irrelevant Content** – Dumping an entire document may include irrelevant text, distracting the model and lowering answer quality.\n",
        "\n",
        "3. **Lack of Targeted Retrieval** – The model doesn’t \"know\" what parts are most relevant to the query, so it can't focus its reasoning effectively.\n",
        "\n",
        "4. **Inefficient Scaling** – As the document base grows, this approach becomes slower, more expensive, and harder to manage.\n",
        "\n",
        "**RAG** solves this by retrieving only the most relevant snippets per query, optimizing both accuracy and efficiency.\n"
      ],
      "metadata": {
        "id": "WiTG18L2UDxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What we will learn:**\n",
        "- PDF parsing and text extraction.\n",
        "- Generating vector representations (embeddings) of text.\n",
        "- Storing and searching these embeddings using a vector database.\n",
        "- Integrating these components to build a simple Q&A system.\n"
      ],
      "metadata": {
        "id": "k2bRltaHUTZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What we will use and what are the parts:**\n",
        "- **Libraries:** `openai` (for embeddings and potentially Q&A models), `pymupdf` (for PDF loading and processing), `faiss-cpu` (for the vector store), and `scikit-learn` (for supporting utilities).\n",
        "- **Parts:**\n",
        "    - **PDF Loader:** Using `pymupdf` to read the content of a PDF file.\n",
        "    - **Text Processor:** Breaking down the PDF content into smaller chunks suitable for embedding.\n",
        "    - **Embeddings Generator:** Using the `openai` library to create vector embeddings for the text chunks.\n",
        "    - **Vector Store:** Using `faiss-cpu` to store the embeddings and perform fast similarity searches to find relevant text chunks based on a query.\n",
        "    - **Question Answering Logic:** (This part would be built on top of the retrieved text chunks, likely using an OpenAI model to generate an answer based on the context).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RDu5xBSGUaPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fitz"
      ],
      "metadata": {
        "id": "tnfssBeDox7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vh2mogApwQl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import fitz  # PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell allows you to upload your local PDF files into the Colab environment.\n",
        "After uploading, they can be processed and embedded for search and retrieval.\n"
      ],
      "metadata": {
        "id": "TtO2uHpABfJa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKMKp7Siq20x"
      },
      "source": [
        "Extracting text from PDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvIwJo0Kpx2m"
      },
      "outputs": [],
      "source": [
        "2.# Upload PDF files to Google Colab\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a helper function `extract_text_from_pdf` that loads a PDF file and extracts all the text into a single string.\n",
        "\n",
        "- `fitz.open(pdf_path)`: Opens the PDF file.\n",
        "- `for page_num in range(len(doc))`: Iterates over all pages.\n",
        "- `doc.load_page(page_num)`: Loads each page by index.\n",
        "- `page.get_text()`: Extracts the text content from the page.\n",
        "- The result is returned as a single concatenated string.\n"
      ],
      "metadata": {
        "id": "HmTnk7a3B5Wy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0Sou93Wqbso"
      },
      "outputs": [],
      "source": [
        "# Function to extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)\n",
        "        text += page.get_text()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded.keys()"
      ],
      "metadata": {
        "id": "7ZdlczDO3BBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines a helper function `extract_text_from_pdf` that loads a PDF file and extracts all the text into a single string.\n",
        "\n",
        "- `fitz.open(pdf_path)`: Opens the PDF file.\n",
        "- `for page_num in range(len(doc))`: Iterates over all pages.\n",
        "- `doc.load_page(page_num)`: Loads each page by index.\n",
        "- `page.get_text()`: Extracts the text content from the page.\n",
        "- The result is returned as a single concatenated string.\n"
      ],
      "metadata": {
        "id": "A1fIdiCQCLBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbW-86HvqiSz"
      },
      "outputs": [],
      "source": [
        "# Extract text from all uploaded PDF files\n",
        "pdf_texts = {}\n",
        "for pdf_file in uploaded.keys():\n",
        "    if pdf_file.endswith(\".pdf\"):\n",
        "        pdf_texts[pdf_file] = extract_text_from_pdf(pdf_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell processes each uploaded PDF file and stores its text content in a dictionary.  \n",
        "Then, it imports tools for vectorization and similarity search:\n",
        "\n",
        "- `uploaded.keys()` contains the names of the uploaded files.\n",
        "- For each `.pdf` file, it calls `extract_text_from_pdf()` and stores the result in `pdf_texts`.\n"
      ],
      "metadata": {
        "id": "UMNTp0vNCO1z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCs8paSPqnkK"
      },
      "outputs": [],
      "source": [
        "# Display extracted text from each PDF file\n",
        "for pdf_file, text in pdf_texts.items():\n",
        "    print(f\"--- {pdf_file} ---\")\n",
        "    print(text[:500])  # Display the first 500 characters of each document\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awIY3qMNq5HG"
      },
      "source": [
        "\n",
        "\n",
        "`TfidfVectorizer` from `sklearn` will be used to convert text into numerical vectors (embeddings).\n",
        "- `faiss` is a fast similarity search library used for indexing and querying embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3jIVfa-qvEM"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import faiss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`TfidfVectorizer` is a tool from `sklearn.feature_extraction.text` that converts a collection of raw documents (like strings of text) into a matrix of TF-IDF features.\n",
        "\n",
        "TF-IDF stands for Term Frequency–Inverse Document Frequency:\n",
        "- **Term Frequency (TF)** measures how frequently a word appears in a document.\n",
        "- **Inverse Document Frequency (IDF)** reduces the weight of common words and increases the importance of rare words across all documents.\n",
        "\n",
        "This combination highlights the most relevant terms in each document, allowing for effective comparison and retrieval.\n",
        "\n",
        "By default, `TfidfVectorizer`:\n",
        "- Converts all text to lowercase\n",
        "- Tokenizes using word boundaries\n",
        "- Removes English stop words (if configured)\n",
        "- Outputs a sparse matrix with one row per document and one column per term\n",
        "\n",
        "In this notebook, it is used to transform the PDF text into numerical vectors that capture the importance of each term for later similarity search.\n"
      ],
      "metadata": {
        "id": "WNLGYo7uCrJ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vELU6PAZrDFt"
      },
      "outputs": [],
      "source": [
        "# Convert text documents to TF-IDF vectors\n",
        "documents = list(pdf_texts.values())\n",
        "vectorizer = TfidfVectorizer()\n",
        "doc_vectors = vectorizer.fit_transform(documents).toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKRygxhrUn7"
      },
      "source": [
        "This step converts each document into a numerical vector that represents the importance of each term in the document relative to the corpus\n",
        "\n",
        "- `dimension = doc_vectors.shape[1]`: Gets the dimensionality of the document vectors (number of features).\n",
        "- `faiss.IndexFlatL2(dimension)`: Initializes a flat index using L2 (Euclidean) distance — a basic and efficient similarity metric.\n",
        "- `index.add(doc_vectors)`: Adds all the document vectors to the FAISS index so they can be searched.\n",
        "\n",
        "After running this cell, the index is ready to perform similarity search with query vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd_qW7SzrGPi"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS index\n",
        "dimension = doc_vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: print what are the dimentions of my database\n",
        "\n",
        "print(f\"The number of documents in the database is: {index.ntotal}\")\n",
        "print(f\"The dimensionality of the vectors in the database is: {index.d}\")"
      ],
      "metadata": {
        "id": "DvOo-91PMh2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Based on the provided code, the \"database\" is not a traditional relational database or a NoSQL database. Instead, it is a **FAISS index** that stores the **TF-IDF vector representations** of the text content extracted from the uploaded PDF files.\n",
        "\n",
        "Here's a breakdown of what the \"database\" (the FAISS index) contains:\n",
        "\n",
        "1.  **Document Vectors:** It stores the numerical vectors generated by the `TfidfVectorizer` for each of the uploaded PDF documents. Each vector is a high-dimensional representation of the document, where the values represent the TF-IDF scores of the terms in that document.\n",
        "2.  **Index Structure:** The FAISS index (`faiss.IndexFlatL2` in this case) provides an efficient structure for performing similarity searches on these vectors. It's optimized for finding the \"nearest neighbors\" (most similar documents) to a given query vector based on the L2 (Euclidean) distance metric.\n",
        "3.  **No Original Text or Metadata:** The FAISS index itself **does not store the original text content** of the PDFs, nor does it store any metadata about the documents (like filenames). It only stores the numerical vectors. When you find similar vectors in the index, you would need to map back to the original documents (using the `pdf_texts` dictionary and the order in `documents` and `doc_vectors`) to retrieve the actual text or filenames.\n",
        "\n",
        "In essence, the \"database\" is a **vector store** specifically designed for fast similarity searching, not for storing and querying structured or unstructured data in the conventional sense."
      ],
      "metadata": {
        "id": "cKXhphIFySLl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uplentTrbD4"
      },
      "source": [
        "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. We create an index using the IndexFlatL2 method, which builds a flat (non-hierarchical) index based on L2 (Euclidean) distance. We then add our document vectors to this index using the add method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF1cXV_yrgO0"
      },
      "source": [
        "# **Searching the index**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function performs a similarity search over the vectorized documents using FAISS.\n",
        "\n",
        "- `query`: A user input string (e.g. a question or topic).\n",
        "- `vectorizer.transform([query])`: Converts the query string into a TF-IDF vector, using the same vectorizer as the documents.\n",
        "- `index.search(query_vector, top_k)`: Searches the FAISS index to find the `top_k` most similar document vectors, returning both:\n",
        "  - `distances`: Similarity scores (lower distance = higher similarity).\n",
        "  - `indices`: Indices of the matching documents in the original list.\n",
        "- `results`: A list of tuples `(document_text, distance_score)` for each result.\n",
        "\n",
        "The function returns the top matching documents with their similarity scores, enabling context retrieval based on user input.\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "TijeIWa5DSn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_documents(query, top_k=1):\n",
        "    query_vector = vectorizer.transform([query]).toarray()\n",
        "    # Ensure top_k does not exceed the number of documents\n",
        "    actual_top_k = min(top_k, len(documents))\n",
        "    distances, indices = index.search(query_vector, actual_top_k)\n",
        "    results = [(documents[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
        "    return results"
      ],
      "metadata": {
        "id": "b0ss4pNgx6LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell runs a test query through the search system and prints out the top matching document texts.\n",
        "\n",
        "- `query = \"...\"`: The question or topic you're searching for.\n",
        "- `search_documents(query)`: Calls the previously defined function to find the most relevant documents.\n",
        "- `for result in search_results`: Loops through the returned list and prints each matching document along with its distance score.\n",
        "\n",
        "This demonstrates the full retrieval flow: from user input → to TF-IDF vector → to FAISS similarity search → to matching text output.\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "_Fr2hHliEzRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEcP8F8QrHZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example query\n",
        "query = \"can I cancel my flight and get refund in elal\"\n",
        "search_results = search_documents(query)\n",
        "for result in search_results:\n",
        "    print(result,end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# till now we only look at the document. we dont do any llm related procedure Let's start call to openai\n",
        "\n",
        "\n",
        "שלפת טקסט מ־PDF\n",
        "✅ הפכת אותו ל־TF-IDF\n",
        "✅ הכנסת אותו ל־FAISS\n",
        "✅ בנית פונקציית חיפוש\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1P7FMlVBxpbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell: Configure OpenAI client for using LLMs"
      ],
      "metadata": {
        "id": "EeCsFctUFEy5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xI281AYRnHFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/"
      ],
      "metadata": {
        "id": "XYEDUFqNpobM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell is here to test my openai is connection and key works fine\n",
        "\n",
        "\n",
        "#response = client.responses.create(\n",
        "#    model=\"gpt-3.5-turbo\",\n",
        "#    instructions=\"You are a coding assistant\",\n",
        "#    input=\"What is RAG?\",\n",
        "#)\n",
        "\n",
        "#print(response.output_text)\n"
      ],
      "metadata": {
        "id": "KoAIhLiZnQN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function sends a question and relevant context to OpenAI's GPT model and returns a generated answer.\n",
        "\n",
        "- `query`: The user's question.\n",
        "- `context`: Text retrieved from documents (e.g., via FAISS).\n",
        "- `llm_model`: The model to use, such as `\"gpt-3.5-turbo\"`.\n",
        "- `your_role`: A system prompt that defines the assistant's behavior (e.g., \"You are a helpful customer support bot\").\n",
        "\n",
        "The prompt includes both the context and the question.  \n",
        "It is sent to the model using `client.chat.completions.create(...)` in a chat format:\n",
        "- `system` message defines the model's persona or instructions.\n",
        "- `user` message provides the actual prompt with the question and context.\n",
        "\n",
        "The response is returned as a string from `response.choices[0].message.content`."
      ],
      "metadata": {
        "id": "CukOffGCHWtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use OpenAI's GPT model to generate a response\n",
        "def generate_response_with_openai(query, context, llm_model, your_role):\n",
        "    prompt = f\"\"\"\n",
        "    Answer the following question based on the provided context.\n",
        "    Context: {context}\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=llm_model,  # or another suitable model\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": your_role},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "IQDYarkirtf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example shows how to generate a response from OpenAI's GPT model using only a plain question, without any retrieved context or system prompt.\n",
        "\n",
        "- `context = \"\"`: No supporting document context is provided.\n",
        "- `your_role = \"\"`: No specific system instruction or role is defined.\n",
        "- `query`: A user-defined question (e.g., about airline refund policies).\n",
        "- `generate_response_with_openai(...)`: The function sends the query to the model and prints the result.\n",
        "\n",
        "This setup allows you to compare how the model performs with vs. without retrieved document context.\n",
        "python\n",
        "\n"
      ],
      "metadata": {
        "id": "TplDX9PzHk5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example 1:  just call openapi\n",
        "context = \"\"\n",
        "your_role = \"\"\n",
        "query = \"can I cancel my flight and get refund in elal\"\n",
        "answer = generate_response_with_openai(query, context,llm_model,your_role )\n",
        "print(\"\\n--- Generated Answer ---\")\n",
        "print(answer)\n",
        "print(len(answer))"
      ],
      "metadata": {
        "id": "5G-EfViUnYhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2:  change the prompt to get better answer\n",
        "your_role = \"you are a travel agent of elal\"\n",
        "answer = generate_response_with_openai(query, context,llm_model,your_role )\n",
        "print(\"\\n--- Generated Answer ---\")\n",
        "print(answer)\n",
        "print(len(answer))"
      ],
      "metadata": {
        "id": "AnqFaP3VsUio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You are a prompt expert!!**"
      ],
      "metadata": {
        "id": "T0265RjJsmpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets add context"
      ],
      "metadata": {
        "id": "1iTN39k0nUhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example demonstrates prompt engineering by manually injecting document context into the GPT prompt.\n",
        "\n",
        "- `context = documents[0]`: The full text of the first uploaded PDF is used as the context.\n",
        "- `generate_response_with_openai(...)`: Sends the question along with this static context to the LLM.\n",
        "- This approach doesn't include retrieval or vector search — it's a direct prompt composition.\n",
        "\n",
        "Although this mimics the \"context injection\" idea behind RAG, it is not true RAG because there’s no dynamic retrieval step based on similarity to the query.\n",
        "\n",
        "You also print the length of both the input context and the output answer to inspect token usage and verbosity.\n"
      ],
      "metadata": {
        "id": "_NYOEqppH9ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 3 - lets add context -this is not a rag- we are prompt-engeneerings\n",
        "context = documents[0]\n",
        "answer = generate_response_with_openai(query, context,llm_model,your_role )\n",
        "print(\"\\n--- Generated Answer ---\")\n",
        "print(answer)\n",
        "print(len(answer))"
      ],
      "metadata": {
        "id": "WUQgEldFa8nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(context)"
      ],
      "metadata": {
        "id": "9DoiTjCKOqh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 4: Full RAG-lite"
      ],
      "metadata": {
        "id": "h-CvdfTNIXb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This example performs a full RAG-style workflow:\n",
        "1. `search_documents(query)`: Retrieves top-k relevant documents using FAISS vector similarity.\n",
        "2. `context = \"\\n\".join([...])`: Concatenates the top results into a single text block as context.\n",
        "3. `generate_response_with_openai(...)`: Sends the query and the retrieved context to OpenAI’s GPT model.\n",
        "4. The answer is printed, along with lengths of context and output for inspection.\n",
        "\n",
        "This is a complete, minimal RAG implementation — enabling the model to ground its response in actual documents rather than hallucinate or guess.\n"
      ],
      "metadata": {
        "id": "ZyRZLjclIXpa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f38961a"
      },
      "outputs": [],
      "source": [
        "#get augmented context\n",
        "search_results = search_documents(query)\n",
        "context = \"\\n\".join([result[0] for result in search_results])\n",
        "print(len(context))\n",
        "answer = generate_response_with_openai(query, context,llm_model,your_role )\n",
        "print(\"\\n--- Generated Answer ---\")\n",
        "print(answer)\n",
        "print(len(answer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# “You just built your first RAG pipeline.From scratch. Like a boss.”"
      ],
      "metadata": {
        "id": "4la3SqLhItSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧪 RAG Evaluation Procedure\n",
        "\n",
        "This notebook implements an evaluation framework for a basic Retrieval-Augmented Generation (RAG) pipeline.  \n",
        "For each user query, we evaluate both system performance and answer quality using the following metrics:\n",
        "\n",
        "### 🔍 Quality Metrics\n",
        "- **Answer Relevance**: Does the generated answer directly address the question?\n",
        "- **Faithfulness**: Is the answer grounded in the retrieved context (i.e., no hallucination)?\n",
        "- **Context Recall**: Does the ground-truth answer appear in the retrieved context?\n",
        "- **Context Precision**: What proportion of the retrieved context actually contains relevant information?\n",
        "- **R**\n"
      ],
      "metadata": {
        "id": "OoqWbvO0QAZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Initialize results DataFrame\n",
        "results_df = pd.DataFrame()\n",
        "\n",
        "# Cost assumptions\n",
        "COST_PER_1K_TOKENS = 0.001  # for embeddings or 0.002 for gpt-3.5-turbo output\n",
        "\n",
        "def evaluate_and_log(query, gold_answer, results_df, top_k=5, model_cost_per_1k_tokens=0.002):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Step 1: retrieve context\n",
        "    search_results = search_documents(query)\n",
        "    context_chunks = [result[0] for result in search_results[:top_k]]\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "\n",
        "    # Step 2: generate answer\n",
        "    response = generate_response_with_openai(query, context, llm_model, your_role)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Step 3: simple heuristic metrics (can be replaced with GPT-based eval)\n",
        "    def fuzzy_match(a, b):\n",
        "        return int(a.lower().strip() in b.lower())\n",
        "\n",
        "    answer_relevance = fuzzy_match(gold_answer, response)\n",
        "    context_recall = fuzzy_match(gold_answer, context)\n",
        "    faithfulness = fuzzy_match(response, context)\n",
        "\n",
        "    # Optional metrics\n",
        "    retrieval_accuracy = any(gold_answer.lower().strip() in chunk.lower() for chunk in context_chunks)\n",
        "    context_precision = sum(gold_answer in chunk for chunk in context_chunks) / len(context_chunks)\n",
        "\n",
        "    # Token cost estimates (very rough)\n",
        "    total_chars = len(context) + len(response)\n",
        "    token_estimate = total_chars / 4  # approx. 4 chars/token\n",
        "    estimated_cost = (token_estimate / 1000) * model_cost_per_1k_tokens\n",
        "\n",
        "    # Logging\n",
        "    row = {\n",
        "        \"query\": query,\n",
        "        \"gold_answer\": gold_answer,\n",
        "        \"generated_answer\": response,\n",
        "        \"context\": context,\n",
        "        \"latency\": round(end_time - start_time, 2),\n",
        "        \"context_len\": len(context),\n",
        "        \"answer_len\": len(response),\n",
        "        \"answer_relevance\": answer_relevance,\n",
        "        \"faithfulness\": faithfulness,\n",
        "        \"context_recall\": context_recall,\n",
        "        \"context_precision\": round(context_precision, 2),\n",
        "        \"retrieval_accuracy\": retrieval_accuracy,\n",
        "        \"token_cost_estimate\": round(estimated_cost, 4)\n",
        "    }\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "2vtabe60P9ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets run our experiments again"
      ],
      "metadata": {
        "id": "t_DyxTWuQVo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define example query and gold answer\n",
        "query = \"can I cancel my flight and get refund in elal\"\n",
        "gold_answer = \"El Al allows flight cancellations under certain conditions.\"\n"
      ],
      "metadata": {
        "id": "E9tiKQgJQmyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "vz8FjVAsQEhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example  evaluation (prompt engineering, no RAG)\n",
        "# Corrected: Added missing closing bracket\n",
        "context_example3 = documents[0] # Use the full text of the first document as context\n",
        "answer_example3 = generate_response_with_openai(query, context_example3, llm_model, your_role)\n",
        "\n",
        "# Example 4 evaluation (Full RAG-lite)\n",
        "search_results_example4 = search_documents(query)\n",
        "context_example4 = \"\\n\".join([result[0] for result in search_results_example4])\n",
        "answer_example4 = generate_response_with_openai(query, context_example4, llm_model, your_role)\n",
        "\n",
        "# Print comparison\n",
        "print(\"--- Comparison of Example 3 vs Example 4 ---\")\n",
        "print(\"\\nQuery:\", query)\n",
        "\n",
        "print(\"\\n--- Example 3 (Prompt Engineering - Full Document Context) ---\")\n",
        "print(\"Context Length:\", len(context_example3))\n",
        "print(\"Generated Answer:\", answer_example3)\n",
        "print(\"Answer Length:\", len(answer_example3))\n",
        "\n",
        "print(\"\\n--- Example 4 (Full RAG-lite - Retrieved Context) ---\")\n",
        "print(\"Context Length:\", len(context_example4))\n",
        "print(\"Generated Answer:\", answer_example4)\n",
        "print(\"Answer Length:\", len(answer_example4))\n",
        "\n",
        "# Summarize the comparison\n",
        "print(\"\\n--- Summary of Comparison ---\")\n",
        "print(f\"Example 3 (Prompt Engineering): Used a concatenation of several documents ({len(context_example3)} characters) as context.\")\n",
        "print(f\"Example 4 (RAG-lite): Used retrieved context ({len(context_example4)} characters), which is likely a subset of the total document(s) based on the query.\")\n",
        "print(f\"Answer Length (Example 3): {len(answer_example3)}\")\n",
        "print(f\"Answer Length (Example 4): {len(answer_example4)}\")\n",
        "print(\"The RAG-lite approach (Example 4) dynamically retrieves context based on the query, which is generally more efficient and effective than providing concatenated documents (Example 3), especially for large documents or multiple documents. This selective context provision helps the LLM focus on relevant information, potentially leading to more accurate and concise answers while using fewer tokens.\")\n",
        "print(\"To perform a more robust evaluation, you would use the `evaluate_and_log` function with ground truth answers and compare metrics like answer relevance, faithfulness, and context precision.\")"
      ],
      "metadata": {
        "id": "idU3-WoeUk1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Example 3: static context from documents[0] ---------\n",
        "print(\"Running Example 3 (static context, no retrieval)\")\n",
        "\n",
        "# Prepare context manually (not using search_documents)\n",
        "context_3 = documents[0]+documents[1]+documents[3]+documents[5] # Use the full text of the first document as context\n",
        "your_role = \"\"  # Or customize if needed\n",
        "\n",
        "# Measure time\n",
        "start = time.time()\n",
        "response_3 = generate_response_with_openai(query, context_3, llm_model, your_role)\n",
        "end = time.time()\n",
        "\n",
        "# Use same evaluation function, but override context & response manually\n",
        "row_3 = {\n",
        "    \"query\": query,\n",
        "    \"gold_answer\": gold_answer,\n",
        "    \"generated_answer\": response_3,\n",
        "    \"context\": context_3,\n",
        "    \"latency\": round(end - start, 2),\n",
        "    \"context_len\": len(context_3),\n",
        "    \"answer_len\": len(response_3),\n",
        "    \"answer_relevance\": int(gold_answer.lower() in response_3.lower()),\n",
        "    \"faithfulness\": int(response_3.lower() in context_3.lower()),\n",
        "    \"context_recall\": int(gold_answer.lower() in context_3.lower()),\n",
        "    \"context_precision\": float(gold_answer in context_3),\n",
        "    \"retrieval_accuracy\": gold_answer.lower() in context_3.lower(),\n",
        "    \"token_cost_estimate\": round((len(context_3) + len(response_3)) / 4 / 1000 * 0.002, 4)\n",
        "}\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([row_3])], ignore_index=True)\n",
        "\n",
        "# --------- Example 4: standard RAG pipeline ---------\n",
        "print(\"Running Example 4 (retrieval + generation)\")\n",
        "results_df = evaluate_and_log(\n",
        "    query=query,\n",
        "    gold_answer=gold_answer,\n",
        "    results_df=results_df,\n",
        "    top_k=5,\n",
        "    model_cost_per_1k_tokens=0.002\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(\"✅ Both examples complete.\")\n",
        "display(results_df.tail(2))\n"
      ],
      "metadata": {
        "id": "PHX20Qv4Qnf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "lets **analize**"
      ],
      "metadata": {
        "id": "maS2Sfv-QZQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic summary stats\n",
        "#results_df.describe(include='all')\n",
        "\n",
        "# Export to CSV\n",
        "#results_df.to_csv(\"rag_evaluation_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "uZEtrYSVQRL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_log_llm(query, gold_answer, results_df, top_k=5, model_cost_per_1k_tokens=0.002):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Retrieve context\n",
        "    search_results = search_documents(query)\n",
        "    context_chunks = [result[0] for result in search_results[:top_k]]\n",
        "    context = \"\\n\".join(context_chunks)\n",
        "\n",
        "    # Generate answer\n",
        "    response = generate_response_with_openai(query, context, \"gpt-3.5-turbo\", \"\")\n",
        "    end_time = time.time()\n",
        "\n",
        "    # LLM-based scoring (instead of fuzzy_match)\n",
        "    answer_relevance = llm_judge_score(query, context, response, gold_answer, \"answer_relevance\")\n",
        "    faithfulness = llm_judge_score(query, context, response, gold_answer, \"faithfulness\")\n",
        "    context_recall = llm_judge_score(query, context, response, gold_answer, \"context_recall\")\n",
        "    context_precision = llm_judge_score(query, context, response, gold_answer, \"context_precision\")\n",
        "    retrieval_accuracy = llm_judge_score(query, context, response, gold_answer, \"retrieval_accuracy\")\n",
        "\n",
        "    # Token estimation\n",
        "    total_chars = len(context) + len(response)\n",
        "    token_estimate = total_chars / 4\n",
        "    estimated_cost = (token_estimate / 1000) * model_cost_per_1k_tokens\n",
        "\n",
        "    # Save to table\n",
        "    row = {\n",
        "        \"query\": query,\n",
        "        \"gold_answer\": gold_answer,\n",
        "        \"generated_answer\": response,\n",
        "        \"context\": context,\n",
        "        \"latency\": round(end_time - start_time, 2),\n",
        "        \"context_len\": len(context),\n",
        "        \"answer_len\": len(response),\n",
        "        \"answer_relevance\": round(answer_relevance, 2),\n",
        "        \"faithfulness\": round(faithfulness, 2),\n",
        "        \"context_recall\": round(context_recall, 2),\n",
        "        \"context_precision\": round(context_precision, 2),\n",
        "        \"retrieval_accuracy\": round(retrieval_accuracy, 2),\n",
        "        \"token_cost_estimate\": round(estimated_cost, 4)\n",
        "    }\n",
        "\n",
        "    results_df = pd.concat([results_df, pd.DataFrame([row])], ignore_index=True)\n",
        "    return results_df\n"
      ],
      "metadata": {
        "id": "p_iDg8_lXP7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def llm_judge_score(query, context, response, gold_answer, metric_type):\n",
        "    \"\"\"\n",
        "    Uses an LLM (like GPT-3.5-turbo) to score a response based on various metrics.\n",
        "\n",
        "    Args:\n",
        "        query (str): The original user query.\n",
        "        context (str): The retrieved context used to generate the response.\n",
        "        response (str): The generated answer from the LLM.\n",
        "        gold_answer (str): The ground truth correct answer.\n",
        "        metric_type (str): The metric to evaluate ('answer_relevance', 'faithfulness',\n",
        "                           'context_recall', 'context_precision', 'retrieval_accuracy').\n",
        "\n",
        "    Returns:\n",
        "        float: The score (between 0 and 1) for the specified metric as judged by the LLM.\n",
        "               Returns 0 if the metric type is invalid or an error occurs.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\n",
        "    if metric_type == \"answer_relevance\":\n",
        "        prompt = f\"\"\"\n",
        "        Evaluate if the following generated answer is relevant to the user query.\n",
        "        Score 1 if it is highly relevant, 0.5 if partially relevant, 0 if not relevant.\n",
        "        User Query: {query}\n",
        "        Generated Answer: {response}\n",
        "        Score:\n",
        "        \"\"\"\n",
        "    elif metric_type == \"faithfulness\":\n",
        "        prompt = f\"\"\"\n",
        "        Evaluate if the following generated answer is supported by the provided context.\n",
        "        Score 1 if the answer is fully supported by the context, 0.5 if partially supported, 0 if not supported (hallucination).\n",
        "        Context: {context}\n",
        "        Generated Answer: {response}\n",
        "        Score:\n",
        "        \"\"\"\n",
        "    elif metric_type == \"context_recall\":\n",
        "        prompt = f\"\"\"\n",
        "        Evaluate if the gold answer is present or implied in the provided context.\n",
        "        Score 1 if the gold answer is fully present in the context, 0.5 if partially present, 0 if not present.\n",
        "        Gold Answer: {gold_answer}\n",
        "        Context: {context}\n",
        "        Score:\n",
        "        \"\"\"\n",
        "    elif metric_type == \"context_precision\":\n",
        "         prompt = f\"\"\"\n",
        "        Evaluate how much of the provided context is relevant to answer the user query.\n",
        "        Score 1 if most of the context is relevant, 0.5 if some is relevant, 0 if little or none is relevant.\n",
        "        User Query: {query}\n",
        "        Context: {context}\n",
        "        Score:\n",
        "        \"\"\"\n",
        "    elif metric_type == \"retrieval_accuracy\":\n",
        "         prompt = f\"\"\"\n",
        "        Evaluate if the provided context contains information necessary to answer the user query.\n",
        "        Score 1 if the context is sufficient to answer the query, 0 if it is not.\n",
        "        User Query: {query}\n",
        "        Context: {context}\n",
        "        Score:\n",
        "        \"\"\"\n",
        "    else:\n",
        "        print(f\"Warning: Invalid metric_type '{metric_type}' for LLM judging.\")\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        llm_response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\", # Using a suitable LLM for judging\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an impartial judge evaluating AI responses. Provide a score between 0 and 1.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=5 # Expecting a single number or simple text\n",
        "        )\n",
        "        score_text = llm_response.choices[0].message.content.strip()\n",
        "        # Try to parse the score as a float\n",
        "        try:\n",
        "            score = float(score_text)\n",
        "            # Clamp the score between 0 and 1 just in case\n",
        "            score = max(0.0, min(1.0, score))\n",
        "            return score\n",
        "        except ValueError:\n",
        "            # If parsing fails, try to interpret simple text like \"1\" or \"0.5\"\n",
        "            if score_text == \"1\": return 1.0\n",
        "            elif score_text == \"0.5\": return 0.5\n",
        "            elif score_text == \"0\": return 0.0\n",
        "            else:\n",
        "                print(f\"Could not parse LLM score '{score_text}' for metric '{metric_type}'. Returning 0.\")\n",
        "                return 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during LLM judging for metric '{metric_type}': {e}\")\n",
        "        return 0.0"
      ],
      "metadata": {
        "id": "61Z-BHiZXM2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVMJybs_XLSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Example 3: static context from documents[0] ---------\n",
        "print(\"Running Example 3 (static context, no retrieval)\")\n",
        "\n",
        "# Prepare context manually (not using search_documents)\n",
        "context_3 = documents[0] # Use the full text of the first document as context\n",
        "your_role = \"\"  # Or customize if needed\n",
        "\n",
        "# Measure time\n",
        "start = time.time()\n",
        "response_3 = generate_response_with_openai(query, context_3, llm_model, your_role)\n",
        "end = time.time()\n",
        "\n",
        "# Use same evaluation function, but override context & response manually\n",
        "row_3 = {\n",
        "    \"query\": query,\n",
        "    \"gold_answer\": gold_answer,\n",
        "    \"generated_answer\": response_3,\n",
        "    \"context\": context_3,\n",
        "    \"latency\": round(end - start, 2),\n",
        "    \"context_len\": len(context_3),\n",
        "    \"answer_len\": len(response_3),\n",
        "    \"answer_relevance\": int(gold_answer.lower() in response_3.lower()),\n",
        "    \"faithfulness\": int(response_3.lower() in context_3.lower()),\n",
        "    \"context_recall\": int(gold_answer.lower() in context_3.lower()),\n",
        "    \"context_precision\": float(gold_answer in context_3),\n",
        "    \"retrieval_accuracy\": gold_answer.lower() in context_3.lower(),\n",
        "    \"token_cost_estimate\": round((len(context_3) + len(response_3)) / 4 / 1000 * 0.002, 4)\n",
        "}\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([row_3])], ignore_index=True)\n",
        "\n",
        "# --------- Example 4: standard RAG pipeline ---------\n",
        "print(\"Running Example 4 (retrieval + generation)\")\n",
        "results_df = evaluate_and_log_llm(\n",
        "    query=query,\n",
        "    gold_answer=gold_answer,\n",
        "    results_df=results_df,\n",
        "    top_k=5,\n",
        "    model_cost_per_1k_tokens=0.002\n",
        ")\n",
        "\n",
        "# Show results\n",
        "print(\"✅ Both examples complete.\")\n",
        "display(results_df.tail(2))\n"
      ],
      "metadata": {
        "id": "BP6Dc7e3XmPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We have a basic flow , but bad metrices. lets try to improve"
      ],
      "metadata": {
        "id": "PX724RM0Z3YJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "id": "eFjAqIPEZ9s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss"
      ],
      "metadata": {
        "id": "vH6yXFvnaQHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# Load a multilingual semantic model\n",
        "embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Step 1: Embed the documents\n",
        "doc_vectors = embed_model.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "# Step 2: Create FAISS index\n",
        "dimension = doc_vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(doc_vectors)\n",
        "\n",
        "# Step 3: Define new search function\n",
        "def search_documents(query, top_k=5):\n",
        "    query_vector = embed_model.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "    return [(documents[i], distances[0][j]) for j, i in enumerate(indices[0])]\n"
      ],
      "metadata": {
        "id": "Ob8jUP-payio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLwT8GCdc9Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------- Example 5: SBERT embedding + GPT-based evaluation ---------\n",
        "print(\"🔎 Running Example 5 - SBERT retrieval + LLM judgment\")\n",
        "\n",
        "# Query and gold answer\n",
        "query = \"can I cancel my flight and get refund in elal\"\n",
        "gold_answer = \"El Al allows flight cancellations under certain conditions.\"\n",
        "\n",
        "# Use SBERT retrieval instead of TF-IDF\n",
        "search_results = search_documents(query, top_k=8)\n",
        "context_chunks = [result[0] for result in search_results]\n",
        "context_5 = \"\\n\".join(context_chunks)\n",
        "\n",
        "# Generate answer with OpenAI\n",
        "start = time.time()\n",
        "response_5 = generate_response_with_openai(query, context_5, \"gpt-3.5-turbo\", \"\")\n",
        "end = time.time()\n",
        "\n",
        "# Evaluate using GPT as judge\n",
        "answer_relevance = llm_judge_score(query, context_5, response_5, gold_answer, \"answer_relevance\")\n",
        "faithfulness = llm_judge_score(query, context_5, response_5, gold_answer, \"faithfulness\")\n",
        "context_recall = llm_judge_score(query, context_5, response_5, gold_answer, \"context_recall\")\n",
        "context_precision = llm_judge_score(query, context_5, response_5, gold_answer, \"context_precision\")\n",
        "retrieval_accuracy = llm_judge_score(query, context_5, response_5, gold_answer, \"retrieval_accuracy\")\n",
        "\n",
        "# Estimate cost\n",
        "total_chars = len(context_5) + len(response_5)\n",
        "token_estimate = total_chars / 4\n",
        "estimated_cost = (token_estimate / 1000) * 0.002\n",
        "\n",
        "# Log results\n",
        "row_5 = {\n",
        "    \"query\": query,\n",
        "    \"gold_answer\": gold_answer,\n",
        "    \"generated_answer\": response_5,\n",
        "    \"context\": context_5,\n",
        "    \"latency\": round(end - start, 2),\n",
        "    \"context_len\": len(context_5),\n",
        "    \"answer_len\": len(response_5),\n",
        "    \"answer_relevance\": round(answer_relevance, 2),\n",
        "    \"faithfulness\": round(faithfulness, 2),\n",
        "    \"context_recall\": round(context_recall, 2),\n",
        "    \"context_precision\": round(context_precision, 2),\n",
        "    \"retrieval_accuracy\": round(retrieval_accuracy, 2),\n",
        "    \"token_cost_estimate\": round(estimated_cost, 4)\n",
        "}\n",
        "\n",
        "results_df = pd.concat([results_df, pd.DataFrame([row_5])], ignore_index=True)\n",
        "\n",
        "# Display last 3 results\n",
        "print(\"✅ Example 5 complete.\")\n",
        "display(results_df.tail(3))\n"
      ],
      "metadata": {
        "id": "hhpLYSz0ceJY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhTMGoEOfa4vSNBq/BCmXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}